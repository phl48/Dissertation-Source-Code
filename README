The original COCO dataset we used and all trained models are prrovided in this cloud links since they are too big to upload on moodle:


1. First, change the dataset path of split-object and run it to seperate the test dataset into large, medium, and small size folder.
2. Install and complie YOLOV4, and test all the trained models: (original source code https://github.com/AlexeyAB/darknet, and all specific steps are also explained in it)
    A. Download the fils in the above cloud links and put all .cfg files into cfg folder in darkent/cfg/
    B. Download the fils in the above cloud links and put all .weights files into weight folder in darkent/backup/
    C. Download the fils in the above cloud links and put .names file into data folder in darkent/data/
    D. Download the fils in the above cloud links and put all .data files into data folder in darkent/data/
    E. Use create_testTxT.py to generate TEXT file for all the path of your test dataset and put it into data folder in darkent/data/
    F. Change the path of train  = data/test-all.txt and valid  = data/test-all.txt in all test_xxx.data  if you generate the different file name in step D.
    G. Use the command to test the models then you will see the mAP and other results on the terminal: 
       Linux ./darknet detector map data/xxx.data cfg/xxx.cfg backup\xxx.weights
       Windows darknet.exe detector map data/xxx.data cfg/xxx.cfg backup\xxx.weights





Below Fuctions are the code we used in our dissertation.

1.MoveImgWithTxT
We have implemented the function of randomly splitting the numbers of images into three datasets and named the python file as MoveImgWithTxT. We import the library of glob, random, argparse, and os in python in this code. Glob is used to return all the file paths that match a specific condition we want. Argparse is a built-in Python module for command item options and parameter parsing. Defining the parameters we need in the program will parse them from sys.argv and automatically generate help and usage information. Here we define the number as the number of images we want to split into another file. Source is the original dataset, and destination is the path where we split the images into. Os is the library that we use to determine whether the file or folder exists.

In all the datasets, each image has one text file with the same name rule. First, we read the whole text files in the source path and split the file names. For example, if there is a text file called abc.txt, there must be an image called abc.png. We split it as abc and txt, then find the same abc name ends with jpg, jpeg, or png file types. Finally, we move the image and text file to another file together.


2.Convert-format 
Convert-format is the file which we read the YOLO format in all the labeled images. There is a space between each value which we use it to split and read the five values as label-line[0], label-line[1], label-line[2], and label-line[3], and label-line[4]. img-width and img-height are the width and height of the original images. Due to the values in YOLO format being all written in normalization, we need to multiply them by img-width and img-height, respectively, to get the real values of labeled objects. Finally, the actual width times actual height will get the area of each labeled object. All of the above codes are defined as images-annotations-info, and we apply them to the split-object code to calculate the area of the labeled objects.


3.Split-object
In Split-object file, we define the three classes, frisbee, kite, and baseball glove, as 0,1, and 2, respectively, which are the numbers we label all the images in the datasets. We also define the sizes of large, medium, and small objects as the above paragraph mentioned. We then create four folders which are large-object, medium-object, small-object, and don't-use. 

Furthermore, we import images-annotations-info from the convert-format code to calculate the area of all the labeled objects and split the images into the corresponding folders.


4.Yolo-format
In Yolo-format file, we define the YOLO format in normalization as images-annotations-yolo-info. It is similar as convert-format file, however, it remains the normalized values instead of transforming them to true ones. We only need the normalized values since we are not calculating the area of labeled objects here but just moving the bounding boxes of the labeled objects. Furthermore, we define a function called write-yolo, which is used in Augment-method code to write all the augmented values in the new text files.


5.Augment-methods
Augment-methods is the file we use all of the data augmentation from the libraries. We define two functions as getTransform and getTransform-cut-grid. GetTransform is the function which we import the methods from Albumentations, while getTransform-cut-grid is the function which we import the methods from Imgaug. 

There are some parameters we can adjust for our methods. P means the percentage of images we want to augment in the datasets. Here we set P=1 as we always enhance the whole datasets. In method 1, multiplier is the density of the noise. limit=45 in method 11 means randomly rotating the images from -45 to 45 degrees. In method 13, 0.06 and size-percent=0.07 mean that we drop 6 percent of all pixels by converting them to 2x2 square black pixels, but do that on a lower-resolution version of the image with 7 percent of the original size. In method 14, ratio=0.2 is the 20 percent of the mask holes to the unit-size. Holes-number-x=3, and holes-number-y=3 are the number of grid units in x and y direction. Random-offset=True means randomly offsetting the grid and value for the dropped pixels.


6.Augment-image
Augment-image is the main file that imports all the functions in other python codes and runs all the data augmentation methods. Line 25 is where we import images-annotations-yolo-info from the Yolo-format file to read the YOLO values in all the text files. From lines 29 to 40 are the codes which we import getTransform from the Augment-methods file to augment the images and change the values of bounding boxes in the text files. Then we save all the new data to the new folders.


7.Cutout-gridmask
Cutout-gridmask is the file we use for Cutout and Gridmask methods and is similar to the Augment-image file. We created this after the Augment-image file, so we did not combine them into one file. Lines 24, 28 and 29 are the switch lines for choosing Cutout or Gridmask. Since the Gridask method will return many results that we do not need, we add ['image'] to return only the array representing the pixel values of the augmented images. From lines 25 to 33, we use the getTransform-cut-grid function from the Augment-methods file and save the augmented images to the new folders. Furthermore, as they both only mask the images, which means the labeled objects will remain in the same position, we only need to copy the old values in text files and attach them with the augmented images as shown from lines 35 to 42.

8.Create_testTxT
This is the code to generate TEXT file which will includ all the paths of labeled images in your dataset for YOLOV4 framework to detect, train, and test the trained models afterwards.
